{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA и предобработка данных\n",
    "\n",
    "Удобно представить данные не только в виде графа, но и в виде квадратной sparse-матрицы взаимодействия пользователей. \n",
    "\n",
    "```python \n",
    "    def _create_interaction_matrix(self, df, target_col='target') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Created square iteration matrix (NaN values updated using transposed values)\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Users interaction history dataset\n",
    "        target_col : str\n",
    "            Label of target column (interaction intensity, time)\n",
    "        Returns\n",
    "        -------\n",
    "        interaction_matrix : pd.Dataframe\n",
    "            Matrix of full users interactions\n",
    "        \"\"\"\n",
    "        interaction_matrix = df[['uid1', 'uid2', target_col]].pivot(index='uid1', columns='uid2')\n",
    "        interaction_matrix.columns = interaction_matrix.columns.droplevel(0)\n",
    "        full_index = interaction_matrix.index.union(interaction_matrix.columns).sort_values()\n",
    "        interaction_matrix = interaction_matrix.reindex(index=full_index, columns=full_index)\n",
    "        interaction_matrix.update(interaction_matrix.T)\n",
    "        if self.idf_normalize:\n",
    "            vc = df[['uid1', 'uid2']].stack().value_counts(normalize=True)\n",
    "            idf = np.log(1 / vc).sort_index()\n",
    "            interaction_matrix = interaction_matrix.multiply(idf, axis=1)\n",
    "        return interaction_matrix.fillna(0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender system model\n",
    "\n",
    "В качестве первой модели для рекомендательной системы я использовал модифицированный kNN с метрикой `cosine_distance`. Для каждой пары пользователей мы может вычислить скалярное произведение, соответственно вычислив матрицу скалярный произведений, отсеяв уже добавленных друзей и отранжировав её можно получить упорядоченный список похожих (в терминах близости векторов) пользователей. Для пользователей, впервые появившихся на тесте, такой метод применить не получится (по крайней мере, сложно на таком объёме данных обобщить пользователей другим способом), сответственно для `cold_users` я использовал вершины с наибольшим количеством связей из графа G.\n",
    "\n",
    "```python \n",
    "    def fit(self):\n",
    "        self.model = linear_kernel(self.train_csr, self.train_csr)\n",
    "\n",
    "    def predict(self, users_list, k=5) -> np.array:\n",
    "        \"\"\"\n",
    "        Computes first k recomendations for users in list\n",
    "        Parameters\n",
    "        ----------\n",
    "        users_list : np.array\n",
    "            An array of users uids who needs recommendations\n",
    "        k : int, optional\n",
    "            The maximum number of predicted elements\n",
    "        Returns\n",
    "        -------\n",
    "        recommendations : np.array\n",
    "            Ordered array with recommended users uids\n",
    "        \"\"\"\n",
    "        if self.ind2uid.isin(users_list).any():\n",
    "            valid_idx = self.uid2ind[users_list]\n",
    "            cold_start = np.argwhere(np.isnan(valid_idx)).flatten()\n",
    "            valid_idx = valid_idx.dropna().astype(int).values\n",
    "            model_recs = self.rec_calc(valid_idx)\n",
    "            users_list = self.ind2uid[valid_idx]\n",
    "            for i, (ind, uid) in tqdm(enumerate(zip(valid_idx, users_list))):\n",
    "                model_recs[i, ind] = -1\n",
    "                if uid in self.neighbors:\n",
    "                    friends_uids = self.neighbors[uid]\n",
    "                    friends_idx = self.uid2ind.loc[friends_uids].dropna().astype(int).values\n",
    "                    model_recs[i, friends_idx] = -1\n",
    "            sorted_recs = model_recs.argsort()[:, ::-1]\n",
    "            final_recs = list(map(lambda x: self.ind2uid.loc[x].values, sorted_recs[:, :k]))\n",
    "            for cold_idx in cold_start:\n",
    "                baseline_recommendation = self.find_popular[:k]\n",
    "                final_recs = np.insert(final_recs, cold_idx, baseline_recommendation, axis=0)\n",
    "            return final_recs\n",
    "        else:\n",
    "            baseline_recommendation = self.find_popular[:k]\n",
    "            return np.array([baseline_recommendation] * len(users_list))\n",
    "        \n",
    "    def rec_calc(self, idx) -> np.array:\n",
    "        recommendations = self.model[idx]\n",
    "        return recommendations\n",
    "```\n",
    "\n",
    "\n",
    "В качестве второй модели для рекомендательной системы я использовал SVD разложение матрицы взаимодействий. Теоретически, после обратного преобразования мы должны получить хорошую аппроксимацию исходной матрицы, но я столкнулся с проблемой (`popularity_bias`?), связанной с тем, что пользователи с наибольшим числом связей перевешивали остальных. Пытался регуляризировать с помощью домножения на `TF-IDF`, добавить коэффициент регуляризации и откалибровать его на тестовой выборке, подобрать нужную метрику. \n",
    "\n",
    "```python \n",
    "    def fit(self):\n",
    "        self.model = TruncatedSVD(random_state=self.random_state)\n",
    "        self.model.fit(self.train_csr)\n",
    "        self.uid1_repres = self.model.transform(self.train_csr)\n",
    "        self.uid2_repres = self.model.components_\n",
    "\n",
    "    def rec_calc(self, idx) -> np.array:\n",
    "        recommendations = np.dot(self.uid1_repres[idx, :], self.uid2_repres[:, :])\n",
    "        return recommendations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проблемы / вопросы / дальнейшие улучшения\n",
    "\n",
    "Основной проблемой остался подбор нужной метрики для восстановления матрицы после SVD. Также постфактум выяснилось, что готовая модель после сереализации весит около 15Гб, дешевле запускать модель с нуля на уже предобработанных датасетах: загрузка сереализованного объекта длится около минуты, построение модели около 20 секунд. Для большого количества предсказаний работает достаточно эффективно, для совсем небольшого задержка для запуска модели уже становится значимой. Основной целью было реализовать систему для эффективной валидации (30% датасета) - на лучшей модели она отрабатывает за 30 секунд, на лучшей эвристике - за минуту.  \n",
    "\n",
    "В качестве следующей модели начал реализовывать обучение с учителем, планирую доделать на выходных - самому интересно, что получится. Пайплайн в принципе понятен: для пар юзеров нагенерить фичей на графах (JaccardCoefficent, ResourceAllocation, AdamicAdar, PreferentialAttachment, CommonNeighbors), после чего добавить аугментаций (например, пары друзей предсказанные kNN и тд) и уже на этом обучить бустинг, предсказания также ранжировать по predict_proba, либо по значению предсказанного intensity. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
